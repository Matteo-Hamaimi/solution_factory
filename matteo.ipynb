{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Id   ProductId          UserId               ProfileName  \\\n",
      "508  509  B000G6RYNE  A3I5AT1101AS3A           Nikolette Tripp   \n",
      "509  510  B000G6RYNE  A22LENLDTGQIU7                R. Yamaoka   \n",
      "516  517  B000G6RYNE  A38KP1POQ191WT  Judy Schinske \"Veronica\"   \n",
      "528  529  B000G6RYNE  A1BXG0K7UD9CTD         MicTrik \"mictrik\"   \n",
      "537  538  B000G6RYNE  A18VDAH788BOAC                      Geeb   \n",
      "\n",
      "     HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
      "508                     1                       2      1  1233360000   \n",
      "509                     4                       7      1  1252713600   \n",
      "516                     0                       1      1  1279065600   \n",
      "528                    20                      27      1  1254009600   \n",
      "537                     1                       3      1  1331856000   \n",
      "\n",
      "                                            Summary  \\\n",
      "508                     Maybe the worst chips ever.   \n",
      "509                   Surprise 1  It's different...   \n",
      "516       I have had better \"Jalapeno Kettle Chips\"   \n",
      "528  They changed the Chips now they taste horrible   \n",
      "537                                  Gone down hill   \n",
      "\n",
      "                                                  Text  \n",
      "508  These are perhaps the worst chips that have ev...  \n",
      "509  Kettle chips now look, feel and taste like Lay...  \n",
      "516  These were nasty, they were so greasy and too ...  \n",
      "528  I once loved these chips and they were the onl...  \n",
      "537  When originally produced in England  these we'...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.metrics import accuracy_score\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from pprint import pprint\n",
    "import gensim.corpora as corpora\n",
    "\n",
    "\n",
    "\n",
    "# Charger le dataset\n",
    "df = pd.read_csv('data.csv',nrows=10000)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Filtrer les données pour le produit spécifique et les scores 1 et 5\n",
    "data = df.loc[(df['ProductId'] == 'B000G6RYNE') & (df['Score'] == 1)]\n",
    "\n",
    "print(data.head())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "508    perhaps worst chip ever gone mouthfor entire l...\n",
      "509    kettle chip look feel taste like lay chip used...\n",
      "516    nasty greasy rich blood plus lacked major flav...\n",
      "528    loved chip chip would buy discovered england b...\n",
      "537    originally produced england best chip ever tas...\n",
      "538    opening numerous bag found none chip flavoring...\n",
      "541    ive bought local supermarket enjoyed although ...\n",
      "543    kettle brand chip used goodoily crunchy flavor...\n",
      "544    absolutely forget confirmed reviewer chip tota...\n",
      "545    chip nasty thought someone spilled drink bag c...\n",
      "547    bought brand trial since tired pingosit claim ...\n",
      "550    ordered kettle chip following flavvorssalt fre...\n",
      "551    purchased low salt indeed low salt however man...\n",
      "554    chip greasy taste burntthere grease bottom bag...\n",
      "555    dont waste money kettle brand potato chip boug...\n",
      "556    defintely tasty madhouse munchies family favor...\n",
      "557    love sour food one cant bear strong sour taste...\n",
      "558    unless really really really like vinegar avoid...\n",
      "560    used eat spicy thai flavor time msg make body ...\n",
      "561    chip greatfor first bag however first bag two ...\n",
      "562    waiting ridiculous amount time case 15 5oz bag...\n",
      "566    terrible cannot believe received item every si...\n",
      "568    kettle branch potato chip new york cheddar goo...\n",
      "576    like order kettle spicy thai chip amazon hard ...\n",
      "582    admit oversalted chip addictive really think o...\n",
      "621    sent 3 week past fresh date stock chip noticea...\n",
      "623    problem order case 12 good half time best kett...\n",
      "Name: Text, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jh/lt1l6hb94_b8vqvq3m3ld1lm0000gn/T/ipykernel_7531/2821783067.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data.drop(['Id', 'ProductId','UserId', 'ProfileName', 'HelpfulnessNumerator', 'HelpfulnessDenominator', 'Time',], axis=1, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Prétraitement des données\n",
    "\n",
    "data.drop(['Id', 'ProductId','UserId', 'ProfileName', 'HelpfulnessNumerator', 'HelpfulnessDenominator', 'Time',], axis=1, inplace=True)\n",
    "data = data.dropna()\n",
    "data['Text'] = data['Text'].apply(lambda x: re.sub('<.*?>', '', x))  # Suppression des balises HTML\n",
    "data['Text'] = data['Text'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))  # Suppression des caractères spéciaux\n",
    "data['Text'] = data['Text'].apply(lambda x: x.lower())  # Mise en minuscules\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "data['Text'] = data['Text'].apply(lambda x: ' '.join(word for word in x.split() if word not in stop_words))  # Suppression des stopwords\n",
    "data['Text'] = data['Text'].apply(lambda x: ' '.join(lemmatizer.lemmatize(word) for word in x.split()))  # Lemmatisation\n",
    "\n",
    "print (data['Text'])\n",
    "\n",
    "\n",
    "\n",
    "# Créer une liste de documents tokenisés à partir de la colonne 'Text'\n",
    "documents = [text.split() for text in data['Text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['perhaps', 'worst', 'chip', 'ever', 'gone', 'mouthfor', 'entire', 'life', 'sour', 'cream', 'onion', 'case', 'chive', 'chip', 'favorite', 'recently', 'kettle', 'brand', 'honey', 'dijon', 'mustard', 'took', 'slot', 'found', 'sour', 'cream', 'onion', 'try', 'themas', 'soon']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(documents[:1][0][:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Créer un dictionnaire\n",
    "dictionary = corpora.Dictionary(documents)\n",
    "\n",
    "\n",
    "# Créer un corpus\n",
    "corpus = [dictionary.doc2bow(doc) for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 8), (11, 1), (12, 1), (13, 1), (14, 4), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 3)]\n"
     ]
    }
   ],
   "source": [
    "print(corpus[:1][0][:30])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meilleurs hyperparamètres :\n",
      "Numéro de topics : 15\n",
      "Nombre de passes : 1500\n",
      "Meilleur score de cohérence : 0.43305609484603763\n",
      "[(0,\n",
      "  '0.007*\"unedible\" + 0.007*\"thai\" + 0.006*\"spicy\" + 0.006*\"item\" + '\n",
      "  '0.005*\"product\" + 0.005*\"know\" + 0.005*\"quality\" + 0.005*\"get\" + '\n",
      "  '0.005*\"stale\" + 0.005*\"probably\"'),\n",
      " (1,\n",
      "  '0.008*\"fried\" + 0.008*\"hole\" + 0.007*\"rancid\" + 0.007*\"first\" + '\n",
      "  '0.007*\"screwed\" + 0.007*\"new\" + 0.006*\"cream\" + 0.006*\"sour\" + '\n",
      "  '0.006*\"onion\" + 0.005*\"bag\"'),\n",
      " (2,\n",
      "  '0.002*\"munchies\" + 0.002*\"tasty\" + 0.002*\"darkburntmore\" + '\n",
      "  '0.002*\"defintely\" + 0.002*\"family\" + 0.002*\"greasyoily\" + 0.002*\"light\" + '\n",
      "  '0.002*\"madhouse\" + 0.002*\"broken\" + 0.002*\"oh\"'),\n",
      " (3,\n",
      "  '0.002*\"munchies\" + 0.002*\"tasty\" + 0.002*\"darkburntmore\" + '\n",
      "  '0.002*\"defintely\" + 0.002*\"family\" + 0.002*\"greasyoily\" + 0.002*\"light\" + '\n",
      "  '0.002*\"madhouse\" + 0.002*\"broken\" + 0.002*\"oh\"'),\n",
      " (4,\n",
      "  '0.010*\"msg\" + 0.006*\"instead\" + 0.006*\"label\" + 0.005*\"ingredient\" + '\n",
      "  '0.005*\"premium\" + 0.005*\"thai\" + 0.005*\"flavor\" + 0.004*\"spicy\" + '\n",
      "  '0.004*\"make\" + 0.004*\"manufacturing\"'),\n",
      " (5,\n",
      "  '0.011*\"overfried\" + 0.006*\"pingosit\" + 0.006*\"trial\" + 0.006*\"luck\" + '\n",
      "  '0.006*\"selling\" + 0.006*\"suffer\" + 0.006*\"tired\" + 0.006*\"natural\" + '\n",
      "  '0.006*\"point\" + 0.006*\"argument\"'),\n",
      " (6,\n",
      "  '0.013*\"vinegar\" + 0.013*\"favorite\" + 0.010*\"nasty\" + 0.009*\"sea\" + '\n",
      "  '0.008*\"spilled\" + 0.008*\"soaked\" + 0.008*\"drink\" + 0.007*\"cut\" + '\n",
      "  '0.007*\"crinkle\" + 0.007*\"hefty\"'),\n",
      " (7,\n",
      "  '0.002*\"munchies\" + 0.002*\"tasty\" + 0.002*\"darkburntmore\" + '\n",
      "  '0.002*\"defintely\" + 0.002*\"family\" + 0.002*\"greasyoily\" + 0.002*\"light\" + '\n",
      "  '0.002*\"madhouse\" + 0.002*\"broken\" + 0.002*\"oh\"'),\n",
      " (8,\n",
      "  '0.013*\"strong\" + 0.010*\"sour\" + 0.007*\"awful\" + 0.007*\"cant\" + '\n",
      "  '0.007*\"smell\" + 0.007*\"love\" + 0.007*\"bear\" + 0.006*\"ever\" + 0.005*\"open\" + '\n",
      "  '0.005*\"food\"'),\n",
      " (9,\n",
      "  '0.002*\"munchies\" + 0.002*\"tasty\" + 0.002*\"darkburntmore\" + '\n",
      "  '0.002*\"defintely\" + 0.002*\"family\" + 0.002*\"greasyoily\" + 0.002*\"light\" + '\n",
      "  '0.002*\"madhouse\" + 0.002*\"broken\" + 0.002*\"oh\"'),\n",
      " (10,\n",
      "  '0.010*\"case\" + 0.009*\"bottom\" + 0.007*\"greasy\" + 0.007*\"waste\" + '\n",
      "  '0.007*\"money\" + 0.007*\"ended\" + 0.007*\"inside\" + 0.007*\"rat\" + '\n",
      "  '0.007*\"stock\" + 0.007*\"unfortunately\"'),\n",
      " (11,\n",
      "  '0.002*\"munchies\" + 0.002*\"tasty\" + 0.002*\"darkburntmore\" + '\n",
      "  '0.002*\"defintely\" + 0.002*\"family\" + 0.002*\"greasyoily\" + 0.002*\"light\" + '\n",
      "  '0.002*\"madhouse\" + 0.002*\"broken\" + 0.002*\"oh\"'),\n",
      " (12,\n",
      "  '0.009*\"gross\" + 0.008*\"numerous\" + 0.008*\"none\" + 0.008*\"happen\" + '\n",
      "  '0.007*\"box\" + 0.007*\"plain\" + 0.007*\"flavoring\" + 0.006*\"opening\" + '\n",
      "  '0.006*\"completely\" + 0.006*\"opened\"'),\n",
      " (13,\n",
      "  '0.010*\"yuck\" + 0.009*\"best\" + 0.008*\"amazon\" + 0.007*\"picture\" + '\n",
      "  '0.007*\"give\" + 0.006*\"potato\" + 0.005*\"case\" + 0.005*\"box\" + 0.005*\"mouse\" '\n",
      "  '+ 0.005*\"email\"'),\n",
      " (14,\n",
      "  '0.011*\"low\" + 0.009*\"many\" + 0.008*\"salt\" + 0.008*\"oil\" + 0.007*\"never\" + '\n",
      "  '0.006*\"purchased\" + 0.006*\"encountered\" + 0.006*\"others\" + '\n",
      "  '0.006*\"unappetizing\" + 0.006*\"dripping\"')]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import LdaMulticore, TfidfModel,LdaModel\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# Création du modèle TF-IDF\n",
    "tfidf_model = TfidfModel(corpus)\n",
    "corpus_tfidf = tfidf_model[corpus]\n",
    "\n",
    "# Paramètres à tester\n",
    "num_topics_list = [5, 10, 15]  # Liste des nombres de topics à tester\n",
    "passes_list = [1000, 1500, 2000]  # Liste des nombres de passes à tester\n",
    "\n",
    "best_coherence_score = -1\n",
    "best_lda_model = None\n",
    "best_num_topics = 0\n",
    "best_passes = 0\n",
    "\n",
    "for num_topics in num_topics_list:\n",
    "    for passes in passes_list:\n",
    "        # Entraînement du modèle LDA\n",
    "        lda_model = LdaModel(corpus=corpus_tfidf,\n",
    "                             id2word=dictionary,\n",
    "                             num_topics=num_topics,\n",
    "                             passes=passes)\n",
    "        \n",
    "        # Calcul de la cohérence pour évaluer le modèle\n",
    "        coherence_model = CoherenceModel(model=lda_model, texts=documents, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_score = coherence_model.get_coherence()\n",
    "        \n",
    "        # Comparaison avec le meilleur score obtenu jusqu'à présent\n",
    "        if coherence_score > best_coherence_score:\n",
    "            best_coherence_score = coherence_score\n",
    "            best_lda_model = lda_model\n",
    "            best_num_topics = num_topics\n",
    "            best_passes = passes\n",
    "\n",
    "# Affichage des meilleurs hyperparamètres et du meilleur modèle\n",
    "print(\"Meilleurs hyperparamètres :\")\n",
    "print(\"Numéro de topics :\", best_num_topics)\n",
    "print(\"Nombre de passes :\", best_passes)\n",
    "print(\"Meilleur score de cohérence :\", best_coherence_score)\n",
    "\n",
    "# Affichage des topics du meilleur modèle\n",
    "pprint(best_lda_model.print_topics())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 8:\n",
      "hole, brand, first, screwed, health, cream, sour, good, style, onion\n",
      "\n",
      "Topic 4:\n",
      "favorite, low, never, cut, feel, hefty, crinkle, many, salt, oil\n",
      "\n",
      "Topic 1:\n",
      "case, ended, money, waste, garbage, cream, cheddar, potato, sour, dont\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Choisissez un document à analyser\n",
    "document_index = 0\n",
    "document = documents[document_index]\n",
    "\n",
    "# Convertissez le document en une représentation vectorielle\n",
    "vector = dictionary.doc2bow(document)\n",
    "\n",
    "# Obtenez la distribution de probabilité des topics pour le document\n",
    "topic_distribution = lda_model[vector]\n",
    "\n",
    "# Triez les topics par ordre décroissant de probabilité\n",
    "sorted_topics = sorted(topic_distribution, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Affichez les mots clés des topics les plus pertinents\n",
    "num_keywords = 10  # Nombre de mots clés à afficher par topic\n",
    "\n",
    "for topic in sorted_topics:\n",
    "    topic_id = topic[0]\n",
    "    topic_keywords = lda_model.show_topic(topic_id, num_keywords)\n",
    "    topic_keywords = [keyword[0] for keyword in topic_keywords]\n",
    "    \n",
    "    print(f\"Topic {topic_id + 1}:\")\n",
    "    print(\", \".join(topic_keywords))\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exactitude :  0.69675\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Charger les données depuis un fichier CSV\n",
    "data = pd.read_csv('data.csv',nrows=20000)\n",
    "\n",
    "# Diviser les données en variables indépendantes (X) et dépendante (y)\n",
    "X = data['Text']\n",
    "y = data['Score']\n",
    "\n",
    "# Diviser les données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Créer une représentation vectorielle des textes en utilisant TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)\n",
    "\n",
    "# Entraîner un modèle de classification (par exemple, SVM linéaire)\n",
    "model = LinearSVC()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Faire des prédictions sur les données de test\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Évaluer l'exactitude du modèle\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Exactitude : \", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.models.bart.modeling_tf_bart because of the following error (look up to see its traceback):\nNo module named 'keras.saving.hdf5_format'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/transformers/utils/import_utils.py:1076\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1075\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1076\u001b[0m     \u001b[39mreturn\u001b[39;00m importlib\u001b[39m.\u001b[39;49mimport_module(\u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39m+\u001b[39;49m module_name, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__name__\u001b[39;49m)\n\u001b[1;32m   1077\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m         level \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[39mreturn\u001b[39;00m _bootstrap\u001b[39m.\u001b[39;49m_gcd_import(name[level:], package, level)\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:883\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/transformers/models/bart/modeling_tf_bart.py:33\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39m# Public API\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodeling_tf_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     34\u001b[0m     DUMMY_INPUTS,\n\u001b[1;32m     35\u001b[0m     TFCausalLanguageModelingLoss,\n\u001b[1;32m     36\u001b[0m     TFModelInputType,\n\u001b[1;32m     37\u001b[0m     TFPreTrainedModel,\n\u001b[1;32m     38\u001b[0m     keras_serializable,\n\u001b[1;32m     39\u001b[0m     unpack_inputs,\n\u001b[1;32m     40\u001b[0m )\n\u001b[1;32m     41\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtf_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m shape_list, stable_softmax\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/transformers/modeling_tf_utils.py:39\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mhuggingface_hub\u001b[39;00m \u001b[39mimport\u001b[39;00m Repository, list_repo_files\n\u001b[0;32m---> 39\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msaving\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mhdf5_format\u001b[39;00m \u001b[39mimport\u001b[39;00m save_attributes_to_hdf5_group\n\u001b[1;32m     40\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mhub\u001b[39;00m \u001b[39mimport\u001b[39;00m convert_file_size_to_int, get_checkpoint_shard_files\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras.saving.hdf5_format'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m pipeline\n\u001b[0;32m----> 2\u001b[0m summarizer \u001b[39m=\u001b[39m pipeline(\u001b[39m\"\u001b[39;49m\u001b[39msummarization\u001b[39;49m\u001b[39m\"\u001b[39;49m, model\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mknkarthick/MEETING_SUMMARY\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      5\u001b[0m \u001b[39m# Supposons que vous ayez un DataFrame appelé 'data' avec une colonne 'texte'\u001b[39;00m\n\u001b[1;32m      6\u001b[0m texte_complet \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(data[\u001b[39m'\u001b[39m\u001b[39mText\u001b[39m\u001b[39m'\u001b[39m][:\u001b[39m10\u001b[39m]\u001b[39m.\u001b[39mtolist())\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/transformers/pipelines/__init__.py:727\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, framework, revision, use_fast, use_auth_token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    723\u001b[0m \u001b[39m# Infer the framework from the model\u001b[39;00m\n\u001b[1;32m    724\u001b[0m \u001b[39m# Forced if framework already defined, inferred if it's None\u001b[39;00m\n\u001b[1;32m    725\u001b[0m \u001b[39m# Will load the correct model if possible\u001b[39;00m\n\u001b[1;32m    726\u001b[0m model_classes \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m\"\u001b[39m: targeted_task[\u001b[39m\"\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m\"\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m: targeted_task[\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m]}\n\u001b[0;32m--> 727\u001b[0m framework, model \u001b[39m=\u001b[39m infer_framework_load_model(\n\u001b[1;32m    728\u001b[0m     model,\n\u001b[1;32m    729\u001b[0m     model_classes\u001b[39m=\u001b[39;49mmodel_classes,\n\u001b[1;32m    730\u001b[0m     config\u001b[39m=\u001b[39;49mconfig,\n\u001b[1;32m    731\u001b[0m     framework\u001b[39m=\u001b[39;49mframework,\n\u001b[1;32m    732\u001b[0m     task\u001b[39m=\u001b[39;49mtask,\n\u001b[1;32m    733\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs,\n\u001b[1;32m    734\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m    735\u001b[0m )\n\u001b[1;32m    737\u001b[0m model_config \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mconfig\n\u001b[1;32m    738\u001b[0m hub_kwargs[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39m_commit_hash\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/transformers/pipelines/base.py:233\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[0;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[1;32m    231\u001b[0m         classes\u001b[39m.\u001b[39mappend(_class)\n\u001b[1;32m    232\u001b[0m \u001b[39mif\u001b[39;00m look_tf:\n\u001b[0;32m--> 233\u001b[0m     _class \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39;49m(transformers_module, \u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mTF\u001b[39;49m\u001b[39m{\u001b[39;49;00marchitecture\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m    234\u001b[0m     \u001b[39mif\u001b[39;00m _class \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    235\u001b[0m         classes\u001b[39m.\u001b[39mappend(_class)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/transformers/utils/import_utils.py:1067\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1065\u001b[0m \u001b[39melif\u001b[39;00m name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_class_to_module\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m   1066\u001b[0m     module \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_module(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_class_to_module[name])\n\u001b[0;32m-> 1067\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39;49m(module, name)\n\u001b[1;32m   1068\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1069\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmodule \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m has no attribute \u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/transformers/utils/import_utils.py:1066\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1064\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_module(name)\n\u001b[1;32m   1065\u001b[0m \u001b[39melif\u001b[39;00m name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_class_to_module\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m-> 1066\u001b[0m     module \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_module(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_class_to_module[name])\n\u001b[1;32m   1067\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(module, name)\n\u001b[1;32m   1068\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/transformers/utils/import_utils.py:1078\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1076\u001b[0m     \u001b[39mreturn\u001b[39;00m importlib\u001b[39m.\u001b[39mimport_module(\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m module_name, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m   1077\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m-> 1078\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1079\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFailed to import \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mmodule_name\u001b[39m}\u001b[39;00m\u001b[39m because of the following error (look up to see its\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1080\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m traceback):\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1081\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.models.bart.modeling_tf_bart because of the following error (look up to see its traceback):\nNo module named 'keras.saving.hdf5_format'"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "summarizer = pipeline(\"summarization\", model=\"knkarthick/MEETING_SUMMARY\")\n",
    "\n",
    "   \n",
    "# Supposons que vous ayez un DataFrame appelé 'data' avec une colonne 'texte'\n",
    "texte_complet = \" \".join(data['Text'][:10].tolist())\n",
    "print(texte_complet)\n",
    "\n",
    "\n",
    "print(summarizer(texte_complet, min_length=50, max_length=200)[0]['summary_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.2117144614458084, 'start': 59, 'end': 84, 'answer': 'gives freedom to the user'}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n",
    "\n",
    "model_name = \"deepset/roberta-base-squad2\"\n",
    "\n",
    "# a) Get predictions\n",
    "nlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\n",
    "QA_input = {\n",
    "    'question': 'Why is model conversion important?',\n",
    "    'context': 'The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.'\n",
    "}\n",
    "res = nlp(QA_input)\n",
    "\n",
    "# b) Load model & tokenizer\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
    "model_name = 'tuner007/pegasus_paraphrase'\n",
    "torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "model = PegasusForConditionalGeneration.from_pretrained(model_name).to(torch_device)\n",
    "\n",
    "def get_response(input_text,num_return_sequences,num_beams):\n",
    "  batch = tokenizer([input_text],truncation=True,padding='longest',max_length=60, return_tensors=\"pt\").to(torch_device)\n",
    "  translated = model.generate(**batch,max_length=60,num_beams=num_beams, num_return_sequences=num_return_sequences, temperature=1.5)\n",
    "  tgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "  return tgt_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The user has freedom.',\n",
       " 'The user has the freedom to use.',\n",
       " 'The user is given freedom.',\n",
       " 'It gives the user freedom.',\n",
       " 'The user has the freedom to use it.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_beams = 10\n",
    "num_return_sequences = 5\n",
    "context = \"gives freedom to the user\"\n",
    "get_response(context,num_return_sequences,num_beams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['High-quality', 'User-friendly', 'Reliable', 'Convenient', 'Secure']\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline(\"zero-shot-classification\",\n",
    "                      model=\"facebook/bart-large-mnli\")\n",
    "texte_complet = \" \".join(data['Text'][:10].tolist())\n",
    "\n",
    "summary_words = [\n",
    "    \"Reliable\",\n",
    "    \"Efficient\",\n",
    "    \"High-quality\",\n",
    "    \"User-friendly\",\n",
    "    \"Innovative\",\n",
    "    \"Convenient\",\n",
    "    \"Fast\",\n",
    "    \"Secure\",\n",
    "    \"Disappointing\",\n",
    "]\n",
    "\n",
    "result = classifier(texte_complet, summary_words)\n",
    "sorted_labels = sorted(result['labels'], key=lambda x: result['scores'][result['labels'].index(x)], reverse=True)\n",
    "print(sorted_labels[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gives freedom to the user\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n",
    "\n",
    "model_name = \"deepset/roberta-base-squad2\"\n",
    "\n",
    "# a) Get predictions\n",
    "nlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\n",
    "QA_input = {\n",
    "    'question': 'Why is model conversion important?',\n",
    "    'context': 'The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.'\n",
    "}\n",
    "res = nlp(QA_input)\n",
    "\n",
    "print(res['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The user has freedom.']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
    "model_name = 'tuner007/pegasus_paraphrase'\n",
    "torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "model = PegasusForConditionalGeneration.from_pretrained(model_name).to(torch_device)\n",
    "\n",
    "def get_response(input_text,num_return_sequences,num_beams):\n",
    "  batch = tokenizer([input_text],truncation=True,padding='longest',max_length=60, return_tensors=\"pt\").to(torch_device)\n",
    "  translated = model.generate(**batch,max_length=60,num_beams=num_beams, num_return_sequences=num_return_sequences, temperature=1.5)\n",
    "  tgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "  return tgt_text\n",
    "\n",
    "num_beams = 10\n",
    "num_return_sequences = 1\n",
    "context = res['answer']\n",
    "get_response(context,num_return_sequences,num_beams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-large\")\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fatima/anaconda3/lib/python3.10/site-packages/transformers/generation_utils.py:1359: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> The positive point is that the sand is very soft and the sand is very\n"
     ]
    }
   ],
   "source": [
    "question = \"What is the best way to get to the airport?\"\n",
    "\n",
    "inputs = tokenizer.encode(question, return_tensors=\"pt\")\n",
    "outputs = model.generate(inputs)\n",
    "response = tokenizer.decode(outputs[0])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
